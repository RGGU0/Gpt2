# -*- coding: utf-8 -*-
"""5 Gpt2.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OGYHBJiR9owMLt5Ts9t7wz9xeGsB-zfx
"""

!pip install accelerate>=0.20.1
!pip install transformers==4.30.1

!pip install datasets transformers

!pip install clearml
!pip install clearml-agent

import datasets
from datasets import load_dataset
datasets = load_dataset("huggingartists/the-beatles")

datasets = datasets["train"].train_test_split(test_size=0.2)

from huggingface_hub import notebook_login

notebook_login()

!apt install git-lfs

import transformers

print(transformers.__version__)

from transformers.utils import send_example_telemetry

send_example_telemetry("language_modeling_from_scratch_notebook", framework="pytorch")

datasets["train"]['text'][0]

from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))

show_random_elements(datasets["train"])

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])

block_size = 18

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=50,
    num_proc=4,
)

tokenizer.decode(lm_datasets["train"][1]["input_ids"])

from transformers import AutoConfig, AutoModelForCausalLM

config = AutoConfig.from_pretrained(model_checkpoint, num_labels=2)
model = AutoModelForCausalLM.from_config(config)

from transformers import Trainer, TrainingArguments

from transformers import AutoTokenizer, DataCollatorForSeq2Seq
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    f"{model_checkpoint}-wikitext2",
    evaluation_strategy="epoch",
    learning_rate=1.372e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=20,
    fp16=True,
    predict_with_generate=True
)

lm_datasetsm = lm_datasets["train"].train_test_split(test_size=0.5)

lm_datasets05 = lm_datasetsm["train"].train_test_split(test_size=0.5)

lm_datasets05

lm_datasets025 = lm_datasets05["test"].train_test_split(test_size=0.5)

lm_datasets0 = lm_datasets025["train"].train_test_split(test_size=0.2)

lm_datasets0

!pip install evaluate

import evaluate
from evaluate import list_evaluation_modules
from evaluate import load

from evaluate import load
accuracy = evaluate.load("accuracy")

import numpy as np

def compute_metrics(eval_preds):
    metric = evaluate.load("accuracy")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

def compute_metrics(eval_preds):
    metric = evaluate.load("accuracy", "precision")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

evaluate.__version__

!pip install rouge_score

metric = evaluate.load("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = np.where(labels != -100, labels, tokenizer.eos_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    return result

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets0["train"],
    eval_dataset=lm_datasets0["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

!pip install git+https://github.com/allegroai/clearml

! export MPLBACKEND=TkAg

import torch
torch.cuda.empty_cache()

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

import nltk

trainer.train()

from clearml import Task, Logger

# Commented out IPython magic to ensure Python compatibility.
# %env CLEARML_WEB_HOST=https://app.clear.ml
# %env CLEARML_API_HOST=https://api.clear.ml
# %env CLEARML_FILES_HOST=https://files.clear.ml
# %env CLEARML_API_ACCESS_KEY=AYJJL49I2C1BT3KQD2N4
# %env CLEARML_API_SECRET_KEY=FGoOPtxeWlOMynvfnEnRW9qybcYusPdAWrkPkdlQMAMjavL8rr

from clearml import Task, Logger

task = Task.init(
    project_name='Gpt2',
    task_name='Gpt2')

task.close()

"""Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"""

!clearml-agent daemon --queue default --stop

!clearml-agent daemon --queue default

x = "Hello how are you Ben "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=20, return_dict_in_generate=True, output_scores=True)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "Hello how are you Ben "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "yesterday I walked along the park "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "yesterday I walked along the park "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "Outside the window I see "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "Outside the window I see "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "I see "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

x = "Can you go "
inputs = tokenizer(x, return_tensors="pt").to('cuda')

model_outputs = model.generate(**inputs, max_new_tokens=40, return_dict_in_generate=True, output_scores=True, pad_token_id=tokenizer.eos_token_id)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

torch.save(model, 'content0')